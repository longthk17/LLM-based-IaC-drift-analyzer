import re
import uuid
from datetime import datetime, timezone

import config
from core.bedrock_sync import sync_data_source_by_repo
from .s3_uploader import clear_repo_output_in_s3, upload_folder_to_s3
from .git_handler import clone_or_pull
from .terraform_parser import process_directory
from .jsonl_writer import write_jsonl_safely


def extract_owner_repo(repo_url: str):
    """L·∫•y owner v√† repo name t·ª´ URL GitHub."""
    match = re.search(
        r"github\.com[:/](?P<owner>[^/]+)/(?P<repo>[^/]+?)(?:\.git)?$", repo_url
    )
    if match:
        return match.group("owner"), match.group("repo")
    return "unknown", "unknown"


def normalize_chunk(chunk, repo_url, commit_sha, timestamp):
    """Chu·∫©n ho√° 1 resource block v·ªÅ format chu·∫©n."""
    owner, repo_name = extract_owner_repo(repo_url)

    return {
        "repo": repo_url,
        "commit": commit_sha,
        "file": chunk.get("file", "unknown"),
        "lines": chunk.get("lines", "0-0"),
        "resource_address": chunk.get("resource_address", "unknown"),
        "resource_type": chunk.get("resource_type", "unknown"),
        "module": chunk.get("module", "root"),
        "account": owner,
        "region": chunk.get("region", "unknown"),
        "content": chunk.get("content", ""),
        "type": "iac_configuration",
        "id": str(uuid.uuid1()),
        "update_at": timestamp,
        "owner": owner,
        "metadata": {
            "repo": repo_url,
            "commit": commit_sha,
            "owner": owner,
            "region": chunk.get("region", "unknown"),
            "account": owner,
        },
    }


def run_drift_analyzer(repos):
    timestamp = datetime.now(timezone.utc).strftime("%Y-%m-%dT%H:%M:%SZ")
    all_chunks = []
    bucket_name = config.OUTPUT_S3_BUCKET

    for repo_url in repos:
        repo_dir, commit_sha = clone_or_pull(repo_url)
        print(f"üîç Processing repo: {repo_url} @ {commit_sha}")

        if repo_dir is None:
            print(f"‚ö†Ô∏è B·ªè qua {repo_url} v√¨ clone th·∫•t b·∫°i.\n")
            continue

        chunks = process_directory(repo_dir)
        normalized_chunks = []

        for chunk in chunks:
            normalized = normalize_chunk(chunk, repo_url, commit_sha, timestamp)
            normalized_chunks.append(normalized)
            all_chunks.append(normalized)

        # Ghi ra th∆∞ m·ª•c ri√™ng theo repo
        _, repo_name = extract_owner_repo(repo_url)
        repo_output_dir = f"output/{repo_name}"
        write_jsonl_safely(normalized_chunks, repo_output_dir, base_name=repo_name)
        print(f"üìÑ {len(normalized_chunks)} chunks written to {repo_output_dir}")

        # ‚úÖ Clear S3 output ch·ªâ cho repo n√†y
        clear_repo_output_in_s3(bucket_name, repo_name)

        # ‚úÖ Upload l·∫°i d·ªØ li·ªáu m·ªõi
        upload_prefix = f"iac_config/{repo_name}"
        result = upload_folder_to_s3(repo_output_dir, bucket_name, upload_prefix)

        if result["status"] == "success":
            print(f"‚òÅÔ∏è Upload ho√†n t·∫•t: {len(result['uploaded'])} file(s)\n")
        else:
            print(f"‚ö†Ô∏è Upload th·∫•t b·∫°i: {result['error']}\n")
            continue

        # ü§ñ Sync v√†o Amazon Bedrock KB
        s3_repo_path = f"s3://{bucket_name}/{upload_prefix}/"
        sync_result = sync_data_source_by_repo(s3_repo_path)
        print(f"ü§ñ Bedrock Sync Result:", sync_result, "\n")

    print(f"‚úÖ Done. T·ªïng c·ªông {len(all_chunks)} chunks ƒë√£ upload l√™n S3.")
    return all_chunks
